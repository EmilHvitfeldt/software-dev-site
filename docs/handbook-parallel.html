<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Parallel computing</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 60px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h2 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h3 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h4 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h5 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h6 {
  padding-top: 65px;
  margin-top: -65px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">USC Software Dev</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Handbook
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="handbook-introduction.html">Introduction</a>
    </li>
    <li>
      <a href="handbook-standards.html">1: Standards</a>
    </li>
    <li>
      <a href="handbook-hpc.html">2: High Performance Computing</a>
    </li>
    <li>
      <a href="handbook-parallel.html">3: parallel computing</a>
    </li>
    <li>
      <a href="handbook-profiling.html">4: Profiling</a>
    </li>
    <li>
      <a href="handbook-slow-patterns.html">5: Slow R patterns</a>
    </li>
    <li>
      <a href="handbook-r_package.html">6: R packages</a>
    </li>
    <li>
      <a href="handbook-testing.html">7: Testing</a>
    </li>
  </ul>
</li>
<li>
  <a href="bioghost-server.html">Bioghost Server</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Parallel computing</h1>

</div>


<p>Before starting, be advised that this chapter is in continuous work-in-progress mode, which translates into potential changes in the ordering of the sections, availability of contents, etc.</p>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>In this chapter we will introduce the reader to parallel computing using R. In particular, we will take a general overview of what is parallel computing, how can it benefit us, and how can it be used in R.</p>
<p>The rest of this chapter develops under the assumption that the reader has some level of knowledge about R fundamentals (types of objects, functions, etc.).</p>
</div>
<div id="what-is-parallel-computing-anyway" class="section level2">
<h2>What is parallel computing, anyway?</h2>
<p>In very simple terms, parallel computing is all about making things run faster. More concrete, while there are plenty of ways of accelerating calculations, parallel computing is all about doing multiple things simultaneously.</p>
<p>Sequential computing, on the other hand, is what we usually see in R. When we make a call to a function, most of the time R is doing calculations using a single processor. While this may not be a problem if your call takes just a couple of seconds, it may be critical if, for example, the program needs to make a significant number of such calls, say 1,000, in order to be completed.</p>
<p>While not a general rule, most of the time computationally intensive programs can be split in such a way that its components can be executed in an isolated way, this is, without relying on the other parts to be completed.</p>
<p>For example, suppose that you have a function that takes a number and multiplies it by 2, let’s call it <code>f</code>:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a>f &lt;-<span class="st"> </span><span class="cf">function</span>(n) n <span class="op">*</span><span class="st"> </span><span class="dv">2</span></span></code></pre></div>
<p>In R, this simple function can be applied seamlessly to a vector, for example:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a>x &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">4</span></span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="kw">f</span>(x)</span></code></pre></div>
<pre><code>## [1] 2 4 6 8</code></pre>
<p>If we were able to see how calculations are taking place in R, we could represent this in the following way:</p>
<div class="figure">
<img src="figure/pll-computing-explained-serial.png" width="600" alt="" />
<p class="caption">Here we are using a single core. The function is applied one element at a time, leaving the other 3 cores without usage.</p>
</div>
<p>Now, in a parallel computing setting, the same function can be applied simultaneously to multiple elements of <code>x</code>, as in the following figure, where the number of cores matches the number of elements/tasks that need to be processed:</p>
<div class="figure">
<img src="figure/pll-computing-explained-parallel.png" width="600" alt="" />
<p class="caption">In this more intelligent way of computation, we are taking full advantage of our computer by using all 4 cores at the same time. This will translate in a reduced computation time which, in the case of complicated/long calculations, can be an important speed gain.</p>
</div>
<p>In principle, this implementation of the function <code>f</code> should take 1/4 of what the original version takes to be applied to <code>x</code>. As the number of function calls increases, or in other words, as the complexity in terms of computational time increases, it makes sense to start thinking about parallel computing. This takes us to the next section.</p>
</div>
<div id="when-is-it-a-good-idea" class="section level2">
<h2>When is it a good idea?</h2>
<p>Parallel computing sounds great, but is it always a good idea to try to <strong>parallelize</strong> your program? The answer is no.</p>
<p>A lot of times we feel pushed towards writing the code as efficiently as possible. Moreover, this is a known problem among software developers, see for example this hilarious reply on <a href="https://www.stackoverflow.com">Stackoverflow</a> regarding “best comment in source code”<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>:</p>
<pre><code>// 
// Dear maintainer:
// 
// Once you are done trying to &#39;optimize&#39; this routine,
// and have realized what a terrible mistake that was,
// please increment the following counter as a warning
// to the next guy:
// 
// total_hours_wasted_here = 42
// </code></pre>
<p>While optimizing a program may be important in some cases, time constraints and readability may be more important in relative terms. As a rule of thumb, you will only want to optimize your code if by doing so the potential speed gains are worthwhile, for example, reducing computation speed to half of the original time in an algorithm that takes more than just a few seconds. Other examples include:</p>
<p>A good idea when:</p>
<ul>
<li><p>You are writing a log-likelihood Function that you need to maximize. Solvers take, for example, at least 5 calls to the objective function so it makes sense to speed up the call.</p></li>
<li><p>Even more relevant than a simple optimization, if the function needs to be called thousands of times like in an MCMC algorithm, then it definitely makes sense to improve speed.</p></li>
<li><p>You are processing chunks of data in which each requires a significant amount of time to be processed.</p></li>
</ul>
<p>Bad idea when:</p>
<ul>
<li><p>The section of the program that you want to speed up already takes a relatively small time to be completed, for example, a few seconds or a fraction of a second.</p></li>
<li><p>The section of the program you are trying to optimize is not the actual bottleneck of the program<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p></li>
</ul>
<p>If your computational problem is reasonable enough to think about code optimization, and furthermore, implementing parallel computing, then the following diagram should be a useful guide to follow:</p>
<div class="figure">
<img src="figure/when_to_parallel.png" alt="" />
<p class="caption">Ask yourself these questions before jumping into HPC!</p>
</div>
<p>If your problem reached the part in which it can be parallelized but there are no tools around for you to use, keep reading, otherwise move to the next chapter, and don’t come back until you have a problem worthy enough to be dealt with parallel computing… just kidding.</p>
</div>
<div id="fundamentals" class="section level2">
<h2>Fundamentals</h2>
<p>Before jumping into HPC with R, let’s take a look at some concepts that are fundamental for the rest of the chapter.</p>
<div id="types-of-parallelisms" class="section level3">
<h3>Types of parallelisms</h3>
<p>A nice way to look at types of computation is through Flynn’s taxonomy:</p>
<div class="figure">
<img src="figure/flynnsTaxonomy.png" alt="" />
<p class="caption">Flynn’s Classical Taxonomy. Source: Introduction to Parallel Computing, Blaise Barney, Lawrence Livermore National Laboratory <a href="https://computing.llnl.gov/tutorials/parallel_comp/#Whatis">(website)</a></p>
</div>
</div>
<div id="what-you-need-to-know-about-hardware" class="section level3">
<h3>What you need to know about Hardware</h3>
<p>One important thing to know is how many resources we have, and resources can be very different across systems. In general, we can talk about a computer’s processing unit, CPU, as a collection of cores which are grouped/arranged in sockets. Moreover, modern CPUs such as those built by intel have what they call multithreaded technology, which in raw terms means a single physical core behaving as multiple ones. The following figure shows a nice illustration of this:</p>
<div class="figure">
<img src="figure/cpu-slurm.png" alt="" />
<p class="caption">Taxonomy of CPUs (Downloaded from de <a href="https://slurm.schedmd.com/mc_support.html" class="uri">https://slurm.schedmd.com/mc_support.html</a>)</p>
</div>
<p>Now, how many cores does your computer has, the parallel package can tell you that:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1"></a>parallel<span class="op">::</span><span class="kw">detectCores</span>()</span></code></pre></div>
<pre><code>## [1] 12</code></pre>
</div>
<div id="hpc-in-r" class="section level3">
<h3>HPC in R</h3>
<p>Loosely, from R’s perspective, we can think of HPC in terms of two, maybe three things:</p>
<ol style="list-style-type: decimal">
<li><p>Big data: How to work with data that doesn’t fit your computer</p></li>
<li><p>Parallel computing: How to take advantage of multiple core systems</p></li>
<li><p>Compiled code: Write your own low-level code (if R doesn’t have it yet…)</p></li>
</ol>
<p>In the case of Big Data, some solutions include:</p>
<ul>
<li><p>Buy a bigger computer/RAM memory (not the best solution!)</p></li>
<li><p>Use out-of-memory storage, i.e., don’t load all your data in the RAM. e.g. The <a href="https://CRAN.R-project.org/package=bigmemory">bigmemory</a>, <a href="https://CRAN.R-project.org/package=data.table">data.table</a>, <a href="https://CRAN.R-project.org/package=HadoopStreaming">HadoopStreaming</a> R packages</p></li>
<li><p>Store it more efficiently, e.g.: Sparse Matrices (take a look at the <code>dgCMatrix</code> objects from the <a href="https://CRAN.R-project.org/package=Matrix">Matrix</a> R package)</p></li>
</ul>
</div>
</div>
<div id="parallel-computing-in-r" class="section level2">
<h2>Parallel computing in R</h2>
<p>As mentioned earlier, R was not designed to work with parallel computing out-of-the-box. While there are some ways to go around this such as:</p>
<ul>
<li><p>Obtaining the R version owned by Microsoft (<a href="https://mran.microsoft.com/">Microsoft R Open</a>), which has some features, and in particular, linear algebra routines compiled in parallel;</p></li>
<li><p>Compiling R with BLAS allowing for parallel computing (a couple of examples <a href="https://www.r-bloggers.com/why-is-r-slow-some-explanations-and-mklopenblas-setup-to-try-to-fix-this/">here</a> and <a href="https://www.r-bloggers.com/compile-r-and-openblas-from-source-guide/">here</a>);</p></li>
<li><p>Getting the opensource version pqR (<a href="http://www.pqr-project.org/">pretty quick R</a>, which at the writing of this has a stable release published on February 19th, 2019);</p></li>
</ul>
<p>When it comes to using “normal” R, there are several alternatives (just take a look at the <a href="https://cran.r-project.org/web/views/HighPerformanceComputing.html">High-Performance Computing Task View</a>).</p>
<p>When you have a task that you need to perform in parallel it comes in two main flavors. The first flavor is what we call <a href="https://en.wikipedia.org/wiki/Embarrassingly_parallel">embarrassingly parallel</a> or perfectly parallel. These are tasks where you need to do one thing many many times over with complete independence between each task. Wrapping 100 presents is a perfectly parallel task. Inviting 4 friends over would make that task almost 5 times faster then if you were to do that task yourself. The second flavor is one massive task that has some sub-parts that could use parallel computing tasks. These tasks often involve iterative tasks and matrix multiplications</p>
<p>here are examples of how to use different packages to speed up embarrassingly parallel tasks.</p>
<p>We will showcase how to perform these tasks using the same task.</p>
<p><strong>Simulating pi</strong></p>
<ul>
<li><p>We know that <span class="math inline">\(\pi = \frac{A}{r^2}\)</span>. We approximate it by randomly adding points <span class="math inline">\(x\)</span> to a square of size 2 centered at the origin.</p></li>
<li><p>So, we approximate <span class="math inline">\(\pi\)</span> as <span class="math inline">\(\Pr\{\|x\| \leq 1\}\times 2^2\)</span></p></li>
</ul>
<p>The R code to do this</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1"></a>pisim &lt;-<span class="st"> </span><span class="cf">function</span>(nsim) {</span>
<span id="cb7-2"><a href="#cb7-2"></a>  ans  &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(nsim <span class="op">*</span><span class="st"> </span><span class="dv">2</span>), <span class="dt">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb7-3"><a href="#cb7-3"></a>  </span>
<span id="cb7-4"><a href="#cb7-4"></a>  ans  &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">rowSums</span>(ans <span class="op">^</span><span class="st"> </span><span class="dv">2</span>))</span>
<span id="cb7-5"><a href="#cb7-5"></a>  (<span class="kw">sum</span>(ans <span class="op">&lt;=</span><span class="st"> </span><span class="dv">1</span>) <span class="op">*</span><span class="st"> </span><span class="dv">4</span>) <span class="op">/</span><span class="st"> </span>nsim</span>
<span id="cb7-6"><a href="#cb7-6"></a>}</span>
<span id="cb7-7"><a href="#cb7-7"></a></span>
<span id="cb7-8"><a href="#cb7-8"></a><span class="kw">pisim</span>(<span class="dv">1000</span>)</span></code></pre></div>
<pre><code>## [1] 3.164</code></pre>
<p>The non-parallel way to run this many times is to use a looping structure such as <code>lapply()</code>, <code>vapply()</code> or <code>purrr::map()</code>.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1"></a><span class="kw">lapply</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>, pisim)</span></code></pre></div>
<pre><code>## [[1]]
## [1] 4
## 
## [[2]]
## [1] 4
## 
## [[3]]
## [1] 2.666667
## 
## [[4]]
## [1] 4
## 
## [[5]]
## [1] 3.2</code></pre>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1"></a><span class="kw">vapply</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>, pisim, <span class="dt">FUN.VALUE =</span> <span class="kw">numeric</span>(<span class="dv">1</span>))</span></code></pre></div>
<pre><code>## [1] 4.0 4.0 4.0 2.0 3.2</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1"></a>purrr<span class="op">::</span><span class="kw">map</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>, pisim)</span></code></pre></div>
<pre><code>## [[1]]
## [1] 0
## 
## [[2]]
## [1] 4
## 
## [[3]]
## [1] 4
## 
## [[4]]
## [1] 4
## 
## [[5]]
## [1] 3.2</code></pre>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1"></a>purrr<span class="op">::</span><span class="kw">map_dbl</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>, pisim)</span></code></pre></div>
<pre><code>## [1] 0.0 4.0 4.0 2.0 3.2</code></pre>
</div>
<div id="embarresinly-parallel" class="section level2">
<h2>Embarresinly parallel</h2>
<div id="the-future.apply-package" class="section level3">
<h3>The future.apply package</h3>
<p>The <a href="https://github.com/HenrikBengtsson/future.apply">future.apply</a> package is a small package based on the <a href="https://github.com/HenrikBengtsson/future">future</a> package to allow for a drop-in replacement for <code>*apply()</code> functions.</p>
<p>If your code has a place where you are using any of the <code>*apply()</code> functions you can simply specify a <a href="https://github.com/HenrikBengtsson/future#controlling-how-futures-are-resolved">plan</a> and prefix the applys with <code>future_</code></p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1"></a><span class="kw">library</span>(future.apply)</span></code></pre></div>
<pre><code>## Loading required package: future</code></pre>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1"></a><span class="kw">plan</span>(multisession) <span class="co">## Run in parallel on local computer</span></span>
<span id="cb19-2"><a href="#cb19-2"></a></span>
<span id="cb19-3"><a href="#cb19-3"></a><span class="kw">future_lapply</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>, <span class="dt">FUN =</span> pisim)</span></code></pre></div>
<pre><code>## [[1]]
## [1] 0
## 
## [[2]]
## [1] 4
## 
## [[3]]
## [1] 2.666667
## 
## [[4]]
## [1] 3
## 
## [[5]]
## [1] 3.2</code></pre>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1"></a><span class="kw">future_vapply</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>, <span class="dt">FUN =</span> pisim, <span class="dt">FUN.VALUE =</span> <span class="kw">numeric</span>(<span class="dv">1</span>))</span></code></pre></div>
<pre><code>## [1] 4.000000 4.000000 2.666667 4.000000 2.400000</code></pre>
</div>
<div id="the-furrr-package" class="section level3">
<h3>The furrr package</h3>
<p>The <a href="https://github.com/DavisVaughan/furrr">furrr</a> package is a small package based on the <a href="https://github.com/HenrikBengtsson/future">future</a> package to allow for a drop-in replacement for purrr’s <code>map()</code> functions.</p>
<p>If your code has a place where you are using any of the <code>purrr::map*()</code> functions you can simply specify a <a href="https://github.com/HenrikBengtsson/future#controlling-how-futures-are-resolved">plan</a> and prefix the applys with <code>future_</code></p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1"></a><span class="kw">library</span>(furrr)</span>
<span id="cb23-2"><a href="#cb23-2"></a><span class="kw">plan</span>(multisession) <span class="co">## Run in parallel on local computer</span></span>
<span id="cb23-3"><a href="#cb23-3"></a></span>
<span id="cb23-4"><a href="#cb23-4"></a><span class="kw">future_map</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>, pisim)</span></code></pre></div>
<pre><code>## [[1]]
## [1] 0
## 
## [[2]]
## [1] 4
## 
## [[3]]
## [1] 4
## 
## [[4]]
## [1] 4
## 
## [[5]]
## [1] 3.2</code></pre>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1"></a><span class="kw">future_map</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>, <span class="op">~</span><span class="st"> </span><span class="kw">pisim</span>(.x))</span></code></pre></div>
<pre><code>## [[1]]
## [1] 4
## 
## [[2]]
## [1] 2
## 
## [[3]]
## [1] 4
## 
## [[4]]
## [1] 4
## 
## [[5]]
## [1] 3.2</code></pre>
</div>
<div id="the-foreach" class="section level3">
<h3>The foreach</h3>
<p>The <a href="https://github.com/RevolutionAnalytics/foreach">foreach</a> package provides a nice looping structure. You start with a call to <code>foreach()</code> where you pass what you want to loop over then <code>%do%</code> into an expression you want to be performed.</p>
<p>In this case, have we passed in the vector <code>1:5</code> and it is passed to <code>pisim()</code>.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1"></a><span class="kw">library</span>(foreach)</span>
<span id="cb27-2"><a href="#cb27-2"></a><span class="kw">foreach</span>(<span class="dt">dat =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>) <span class="op">%do%</span><span class="st"> </span>{</span>
<span id="cb27-3"><a href="#cb27-3"></a>    <span class="kw">pisim</span>(dat)</span>
<span id="cb27-4"><a href="#cb27-4"></a>}</span></code></pre></div>
<pre><code>## [[1]]
## [1] 0
## 
## [[2]]
## [1] 4
## 
## [[3]]
## [1] 4
## 
## [[4]]
## [1] 4
## 
## [[5]]
## [1] 4</code></pre>
<p>if you want to perform the same function multiple times you simply pass foreach a vector with a length of the desired number of iterations. This is easily done with <code>seq_len()</code></p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1"></a><span class="kw">foreach</span>(<span class="dt">dat =</span> <span class="kw">seq_len</span>(<span class="dv">5</span>)) <span class="op">%do%</span><span class="st"> </span>{</span>
<span id="cb29-2"><a href="#cb29-2"></a>    <span class="kw">pisim</span>(<span class="dv">10000</span>)</span>
<span id="cb29-3"><a href="#cb29-3"></a>}</span></code></pre></div>
<pre><code>## [[1]]
## [1] 3.1044
## 
## [[2]]
## [1] 3.1588
## 
## [[3]]
## [1] 3.1376
## 
## [[4]]
## [1] 3.1296
## 
## [[5]]
## [1] 3.1524</code></pre>
<p><code>foreach()</code> comes with a <code>.combine</code> argument which specifies how the result from each iteration should be combined. <code>c()</code> works well for concatenating vectors</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1"></a><span class="kw">foreach</span>(<span class="dt">dat =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>, <span class="dt">.combine =</span> c) <span class="op">%do%</span><span class="st"> </span>{</span>
<span id="cb31-2"><a href="#cb31-2"></a>    <span class="kw">pisim</span>(dat)</span>
<span id="cb31-3"><a href="#cb31-3"></a>}</span></code></pre></div>
<pre><code>## [1] 4.0 4.0 4.0 4.0 3.2</code></pre>
<p>but you can also use functions like <code>rbind()</code> if your output is a data.frame and want them combined. (This example is not a very efficient, but it illustrates a broader use-case)</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1"></a><span class="kw">foreach</span>(<span class="dt">dat =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>, <span class="dt">.combine =</span> rbind) <span class="op">%do%</span><span class="st"> </span>{</span>
<span id="cb33-2"><a href="#cb33-2"></a>    <span class="kw">data.frame</span>(<span class="dt">pi =</span> <span class="kw">pisim</span>(dat))</span>
<span id="cb33-3"><a href="#cb33-3"></a>}</span></code></pre></div>
<pre><code>##         pi
## 1 0.000000
## 2 4.000000
## 3 2.666667
## 4 4.000000
## 5 3.200000</code></pre>
</div>
<div id="the-parallel-package" class="section level3">
<h3>The parallel package</h3>
<p><strong>parallel</strong>: R package that provides ‘[s]upport for parallel computation, including random-number generation’.</p>
<ol style="list-style-type: decimal">
<li><p>Create a cluster:</p>
<ol style="list-style-type: lower-alpha">
<li><p>PSOCK Cluster: <code>makePSOCKCluster</code>: Creates brand new R Sessions (so nothing is inherited from the master), even in other computers!</p></li>
<li><p>Fork Cluster: <code>makeForkCluster</code>: Using OS <a href="https://en.wikipedia.org/wiki/Fork_(system_call)">Forking</a>, copies the current R session locally (so everything is inherited from the master up to that point). Not available on Windows.</p></li>
<li><p>Other: <code>makeCluster</code> passed to <strong>snow</strong></p></li>
</ol></li>
<li><p>Copy/prepare each R session:</p>
<ol style="list-style-type: lower-alpha">
<li><p>Copy objects with <code>clusterExport</code></p></li>
<li><p>Pass expressions with <code>clusterEvalQ</code></p></li>
<li><p>Set a seed</p></li>
</ol></li>
<li><p>Do your call:</p>
<ol style="list-style-type: lower-alpha">
<li><p><code>mclapply</code>, <code>mcmapply</code> if you are using <strong>Fork</strong></p></li>
<li><p><code>parApply</code>, <code>parLapply</code>, etc. if you are using <strong>PSOCK</strong></p></li>
</ol></li>
<li><p>Stop the cluster with <code>clusterStop</code></p></li>
</ol>
<div id="parallel-example-1-simulating-pi" class="section level4">
<h4>parallel example 1: Simulating <span class="math inline">\(\pi\)</span></h4>
<p>To be able to use a function in the parallel framework we need to do a small rewrite of the function such that it starts with an i argument.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1"></a>parallel_pisim &lt;-<span class="st"> </span><span class="cf">function</span>(i, nsim) {</span>
<span id="cb35-2"><a href="#cb35-2"></a>  ans  &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(nsim <span class="op">*</span><span class="st"> </span><span class="dv">2</span>), <span class="dt">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb35-3"><a href="#cb35-3"></a>  </span>
<span id="cb35-4"><a href="#cb35-4"></a>  ans  &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">rowSums</span>(ans <span class="op">^</span><span class="st"> </span><span class="dv">2</span>))</span>
<span id="cb35-5"><a href="#cb35-5"></a>  (<span class="kw">sum</span>(ans <span class="op">&lt;=</span><span class="st"> </span><span class="dv">1</span>) <span class="op">*</span><span class="st"> </span><span class="dv">4</span>) <span class="op">/</span><span class="st"> </span>nsim</span>
<span id="cb35-6"><a href="#cb35-6"></a>}</span>
<span id="cb35-7"><a href="#cb35-7"></a></span>
<span id="cb35-8"><a href="#cb35-8"></a><span class="co"># 1. CREATING A CLUSTER</span></span>
<span id="cb35-9"><a href="#cb35-9"></a><span class="kw">library</span>(parallel)</span>
<span id="cb35-10"><a href="#cb35-10"></a>cl &lt;-<span class="st"> </span><span class="kw">makePSOCKcluster</span>(<span class="dv">10</span>)</span>
<span id="cb35-11"><a href="#cb35-11"></a></span>
<span id="cb35-12"><a href="#cb35-12"></a><span class="co"># 2. PREPARING THE CLUSTER</span></span>
<span id="cb35-13"><a href="#cb35-13"></a><span class="kw">clusterSetRNGStream</span>(cl, <span class="dv">123</span>)</span>
<span id="cb35-14"><a href="#cb35-14"></a></span>
<span id="cb35-15"><a href="#cb35-15"></a><span class="co"># Number of simulations we want each time to run</span></span>
<span id="cb35-16"><a href="#cb35-16"></a>nsim &lt;-<span class="st"> </span><span class="fl">1e5</span></span>
<span id="cb35-17"><a href="#cb35-17"></a></span>
<span id="cb35-18"><a href="#cb35-18"></a><span class="co"># We need to make -nsim- and -pisim- available to the</span></span>
<span id="cb35-19"><a href="#cb35-19"></a><span class="co"># cluster</span></span>
<span id="cb35-20"><a href="#cb35-20"></a><span class="kw">clusterExport</span>(cl, <span class="kw">c</span>(<span class="st">&quot;nsim&quot;</span>, <span class="st">&quot;parallel_pisim&quot;</span>))</span>
<span id="cb35-21"><a href="#cb35-21"></a></span>
<span id="cb35-22"><a href="#cb35-22"></a><span class="co"># 3. DO YOUR CALL</span></span>
<span id="cb35-23"><a href="#cb35-23"></a><span class="kw">parLapply</span>(cl, <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>, parallel_pisim, <span class="dt">nsim =</span> nsim)</span></code></pre></div>
<pre><code>## [[1]]
## [1] 3.14696
## 
## [[2]]
## [1] 3.1378
## 
## [[3]]
## [1] 3.14236
## 
## [[4]]
## [1] 3.14052
## 
## [[5]]
## [1] 3.13144</code></pre>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1"></a><span class="kw">parSapply</span>(cl, <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>, parallel_pisim, <span class="dt">nsim =</span> nsim)</span></code></pre></div>
<pre><code>## [1] 3.14420 3.14276 3.13592 3.14480 3.14628</code></pre>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1"></a><span class="co"># 4. STOP THE CLUSTER</span></span>
<span id="cb39-2"><a href="#cb39-2"></a><span class="kw">stopCluster</span>(cl)</span></code></pre></div>
<p>Please note that parallel does have a <code>parVapply()</code> so you need to be really careful with your output.</p>
</div>
<div id="parallel-example-2-parallel-rng" class="section level4">
<h4>parallel example 2: Parallel RNG</h4>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1"></a><span class="co"># 1. CREATING A CLUSTER</span></span>
<span id="cb40-2"><a href="#cb40-2"></a>cl &lt;-<span class="st"> </span><span class="kw">makePSOCKcluster</span>(<span class="dv">2</span>)    </span>
<span id="cb40-3"><a href="#cb40-3"></a></span>
<span id="cb40-4"><a href="#cb40-4"></a><span class="co"># 2. PREPARING THE CLUSTER</span></span>
<span id="cb40-5"><a href="#cb40-5"></a><span class="kw">clusterSetRNGStream</span>(cl, <span class="dv">123</span>) <span class="co"># Equivalent to `set.seed(123)`</span></span>
<span id="cb40-6"><a href="#cb40-6"></a></span>
<span id="cb40-7"><a href="#cb40-7"></a><span class="co"># 3. DO YOUR CALL</span></span>
<span id="cb40-8"><a href="#cb40-8"></a>ans &lt;-<span class="st"> </span><span class="kw">parSapply</span>(cl, <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>, <span class="cf">function</span>(x) <span class="kw">runif</span>(<span class="fl">1e3</span>))</span>
<span id="cb40-9"><a href="#cb40-9"></a>(ans0 &lt;-<span class="st"> </span><span class="kw">var</span>(ans))</span></code></pre></div>
<pre><code>##               [,1]          [,2]
## [1,]  0.0861888293 -0.0001633431
## [2,] -0.0001633431  0.0853841838</code></pre>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1"></a><span class="co"># I want to get the same!</span></span>
<span id="cb42-2"><a href="#cb42-2"></a><span class="kw">clusterSetRNGStream</span>(cl, <span class="dv">123</span>)</span>
<span id="cb42-3"><a href="#cb42-3"></a>ans1 &lt;-<span class="st"> </span><span class="kw">var</span>(<span class="kw">parSapply</span>(cl, <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>, <span class="cf">function</span>(x) <span class="kw">runif</span>(<span class="fl">1e3</span>)))</span>
<span id="cb42-4"><a href="#cb42-4"></a></span>
<span id="cb42-5"><a href="#cb42-5"></a>ans0 <span class="op">-</span><span class="st"> </span>ans1 <span class="co"># A matrix of zeros</span></span></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]    0    0
## [2,]    0    0</code></pre>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1"></a><span class="co"># 4. STOP THE CLUSTER</span></span>
<span id="cb44-2"><a href="#cb44-2"></a><span class="kw">stopCluster</span>(cl)</span></code></pre></div>
<p>In the case of <code>makeForkCluster</code></p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1"></a><span class="co"># 1. CREATING A CLUSTER</span></span>
<span id="cb45-2"><a href="#cb45-2"></a><span class="kw">library</span>(parallel)</span>
<span id="cb45-3"><a href="#cb45-3"></a></span>
<span id="cb45-4"><a href="#cb45-4"></a><span class="co"># The fork cluster will copy the -nsims- object</span></span>
<span id="cb45-5"><a href="#cb45-5"></a>nsims &lt;-<span class="st"> </span><span class="fl">1e3</span></span>
<span id="cb45-6"><a href="#cb45-6"></a>cl    &lt;-<span class="st"> </span><span class="kw">makeForkCluster</span>(<span class="dv">2</span>)    </span>
<span id="cb45-7"><a href="#cb45-7"></a></span>
<span id="cb45-8"><a href="#cb45-8"></a><span class="co"># 2. PREPARING THE CLUSTER</span></span>
<span id="cb45-9"><a href="#cb45-9"></a><span class="kw">RNGkind</span>(<span class="st">&quot;L&#39;Ecuyer-CMRG&quot;</span>)</span>
<span id="cb45-10"><a href="#cb45-10"></a><span class="kw">set.seed</span>(<span class="dv">123</span>) </span>
<span id="cb45-11"><a href="#cb45-11"></a></span>
<span id="cb45-12"><a href="#cb45-12"></a><span class="co"># 3. DO YOUR CALL</span></span>
<span id="cb45-13"><a href="#cb45-13"></a>ans &lt;-<span class="st"> </span><span class="kw">do.call</span>(cbind, <span class="kw">mclapply</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>, <span class="cf">function</span>(x) {</span>
<span id="cb45-14"><a href="#cb45-14"></a>  <span class="kw">runif</span>(nsims) <span class="co"># Look! we use the nsims object!</span></span>
<span id="cb45-15"><a href="#cb45-15"></a>               <span class="co"># This would have failed in makePSOCKCluster</span></span>
<span id="cb45-16"><a href="#cb45-16"></a>               <span class="co"># if we didn&#39;t copy -nsims- first.</span></span>
<span id="cb45-17"><a href="#cb45-17"></a>  }))</span>
<span id="cb45-18"><a href="#cb45-18"></a>(ans0 &lt;-<span class="st"> </span><span class="kw">var</span>(ans))</span></code></pre></div>
<pre><code>##            [,1]       [,2]
## [1,] 0.08538418 0.00239079
## [2,] 0.00239079 0.08114219</code></pre>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1"></a><span class="co"># Same sequence with same seed</span></span>
<span id="cb47-2"><a href="#cb47-2"></a><span class="kw">set.seed</span>(<span class="dv">123</span>) </span>
<span id="cb47-3"><a href="#cb47-3"></a>ans1 &lt;-<span class="st"> </span><span class="kw">var</span>(<span class="kw">do.call</span>(cbind, <span class="kw">mclapply</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>, <span class="cf">function</span>(x) <span class="kw">runif</span>(nsims))))</span>
<span id="cb47-4"><a href="#cb47-4"></a></span>
<span id="cb47-5"><a href="#cb47-5"></a>ans0 <span class="op">-</span><span class="st"> </span>ans1 <span class="co"># A matrix of zeros</span></span></code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]    0    0
## [2,]    0    0</code></pre>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1"></a><span class="co"># 4. STOP THE CLUSTER</span></span>
<span id="cb49-2"><a href="#cb49-2"></a><span class="kw">stopCluster</span>(cl)</span></code></pre></div>
</div>
</div>
</div>
<div id="allowing-parallel-computing-in-your-r-package" class="section level2">
<h2>Allowing parallel computing in your R package</h2>
<p>If you are writing a package that has functions that perform tasks that are done in parallel it is advised that you use <a href="https://github.com/RevolutionAnalytics/foreach">foreach</a> or <a href="https://github.com/HenrikBengtsson/future">future</a> in your package. This will mean that you as the package author can decide what parts of your code can run in parallel, and the user can decide how it should be run.</p>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<ul>
<li><a href="https://rstudio.com/resources/rstudioconf-2020/parallel-computing-with-r-using-foreach-future-and-other-packages/">Parallel computing with R using foreach, future, and other packages - Bryan Lewis</a></li>
</ul>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Read the original post (now closed) <a href="https://stackoverflow.com/questions/184618/what-is-the-best-comment-in-source-code-you-have-ever-encountered">here</a>.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>For more about how to identify code bottlenecks, take a look at the Profiling section of this book <a href="#profile-benchmark">here</a>.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
